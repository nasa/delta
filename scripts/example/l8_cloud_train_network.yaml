train:
  network:
    layers:
     - Input:
         shape: [~, ~, num_bands]
     - Conv2D:
          filters: 16
          kernel_size: [3, 3]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: c1
     - Dropout:
         rate: 0.2
     - MaxPool2D:
     - Conv2D:
          filters: 32
          kernel_size: [3, 3]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: c2
     - Dropout:
         rate: 0.2
     - MaxPool2D:
     - Conv2D:
          filters: 64
          kernel_size: [3, 3]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: c3
     - Dropout:
         rate: 0.2
     - MaxPool2D:
     - Conv2D:
          filters: 128
          kernel_size: [3, 3]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: c4
     - UpSampling2D:
     - Conv2D:
          filters: 64
          kernel_size: [2, 2]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: u3
     - Concatenate:
         inputs: [c3, u3]
     - Dropout:
         rate: 0.2
     - Conv2D:
          filters: 64
          kernel_size: [3, 3]
          padding: same
     - UpSampling2D:
     - Conv2D:
          filters: 32
          kernel_size: [2, 2]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: u2
     - Concatenate:
         inputs: [c2, u2]
     - Dropout:
         rate: 0.2
     - Conv2D:
          filters: 32
          kernel_size: [3, 3]
          padding: same
     - UpSampling2D:
     - Conv2D:
          filters: 16
          kernel_size: [2, 2]
          padding: same
     - BatchNormalization:
     - Activation:
         activation: relu
         name: u1
     - Concatenate:
         inputs: [c1, u1]
     - Dropout:
         rate: 0.2
     - Conv2D:
         filters: 7
         kernel_size: [3, 3]
         activation: linear
         padding: same
     - Softmax:
         axis: 3
